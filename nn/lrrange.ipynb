{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 79.16 MB\n",
      "Memory usage after optimization is: 27.23 MB\n",
      "Decreased by 65.6%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(r'C:\\Users\\soube\\OneDrive\\Desktop\\Hammudi\\Bachelorarbeit\\Repository\\AP-rent-determination\\nn.py'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from functions import *\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Selecting the DataSource\n",
    "dataSource = r\"C:\\Users\\soube\\OneDrive\\Desktop\\Hammudi\\Bachelorarbeit\\Repository\\AP-rent-determination\\students_data\\cleaned_data_conf_with_IQR_removal.csv\"\n",
    "\n",
    "# Selecting columns to drop out of featureList and creating LabelList\n",
    "featureDropList = [\"_id\", \"observationDate\", \"state\", \"city\", \"AP_community\", \"community_id\",\"postcode\", \"base_rent\", \"qm2_rent\", \"DE_qm2_rent\"]\n",
    "LabelList = [\"qm2_rent\"]\n",
    "\n",
    "# Create DataFrame from DataSource\n",
    "try: \n",
    "    data = import_data(dataSource)\n",
    "except:\n",
    "    data = pd.read_csv(dataSource)\n",
    "data = data[data[\"state\"] == \"Sachsen-Anhalt\"]\n",
    "\n",
    "\n",
    "#data.drop(data.filter(regex = \"second\"), axis = 1, inplace = True)\n",
    "#data.drop(data.filter(regex = \"third\"), axis = 1, inplace = True)\n",
    "\n",
    "numerical = data.select_dtypes(include=['float16','float32','float64','int8','int16','int32','int64']).columns\n",
    "#Normalize the data by MinMaxScaling\n",
    "data.loc[:,numerical] = preprocessing.MinMaxScaler().fit_transform(data.loc[:,numerical])\n",
    "\n",
    "# Create feature and label lists\n",
    "y = data[LabelList]\n",
    "X = data.drop(featureDropList, axis = 1)\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "import seaborn as sns\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = X_train.shape[1]))\n",
    "model.add(Dense(180, activation = 'relu'))\n",
    "model.add(Dense(180, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "loss = 'mse'\n",
    "metrics = ['mae']\n",
    "# For regression MSE or MAE are good loss functions\n",
    "model.compile(optimizer = \"adam\", loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m lr_range_tuner \u001b[39m=\u001b[39m LRRangeTuner(base_lr_range, max_lr_range, step_size)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Pass the callback to the fit() method when training your model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, callbacks\u001b[39m=\u001b[39;49m[lr_range_tuner])\n",
      "File \u001b[1;32mc:\\Users\\soube\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn [13], line 21\u001b[0m, in \u001b[0;36mLRRangeTuner.on_epoch_begin\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_begin\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_range_tuner\u001b[39m.\u001b[39;49mon_epoch_begin(epoch, logs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'optimizer'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "class LRRangeTuner(Callback):\n",
    "    def __init__(self, base_lr_range, max_lr_range, step_size):\n",
    "        self.base_lr_range = base_lr_range\n",
    "        self.max_lr_range = max_lr_range\n",
    "        self.step_size = step_size\n",
    "        self.lr_range_tuner = LearningRateScheduler(self.lr_range_tuner_step)\n",
    "\n",
    "    def lr_range_tuner_step(self, epoch):\n",
    "        if epoch % self.step_size == 0:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            lr_range = (self.base_lr_range + self.max_lr_range) / 2\n",
    "            K.set_value(self.model.optimizer.lr, lr_range)\n",
    "            print('New learning rate:', lr_range)\n",
    "        return K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % self.step_size == 0:\n",
    "            self.lr_range_tuner.on_epoch_begin(epoch, logs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.step_size == 0:\n",
    "            self.lr_range_tuner.on_epoch_end(epoch, logs)\n",
    "\n",
    "# Define the base and maximum learning rate range\n",
    "base_lr_range = 0.001\n",
    "max_lr_range = 0.005\n",
    "\n",
    "# Define the step size for changing the learning rate\n",
    "step_size = 5\n",
    "\n",
    "# Instantiate the LRRangeTuner callback\n",
    "lr_range_tuner = LRRangeTuner(base_lr_range, max_lr_range, step_size)\n",
    "\n",
    "# Pass the callback to the fit() method when training your model\n",
    "model.fit(X_train, y_train, callbacks=[lr_range_tuner])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ee4b56c58f6d37e4778d9818fb4820583e8ad76b15ea0907f514be591be9833"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
